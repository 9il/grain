{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with grain\n",
    "\n",
    "grain is dynamic autograd library on CPU/CUDA devices for D language. While grain's autograd mechanism is dynamic, D compilers can strongly optimize  and validate your scripts statically. This combination gives you great experience.\n",
    "\n",
    "## how to run grain in jupyter\n",
    "\n",
    "This installation guide is tested with\n",
    "\n",
    "- dmd 2.081.2 (recommend [the official installer](https://dlang.org/install.html))\n",
    "- jupyter 4.4.0 (recommend anaconda)\n",
    "\n",
    "and install d kernel and engine as follows\n",
    "``` bash\n",
    "git clone https://github.com/ShigekiKarita/grain\n",
    "cd grain\n",
    "jupyter kernelspec install ./example/third/jupyterd --user\n",
    "dub build --config=jupyterd --compiler=dmd\n",
    "export PATH=`pwd`:$PATH\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "Then you can create a new notebook with 'd' kernel or open existing 'd' notebooks.\n",
    "\n",
    "## expected readers\n",
    "\n",
    "Generally this tutorial is for D users. You can learn D easility in https://tour.dlang.org/tour/ Optionally, experiences of numerical library in D (mir) or in python (numpy, pytorch) help you understanding grain.\n",
    "\n",
    "## reference\n",
    "\n",
    "- Main topics in this tutorial are derived from https://pytorch.org/tutorials\n",
    "\n",
    "## TODO\n",
    "\n",
    "- DUB extension for drepl like `%load_dub grain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mir.ndslice.Slice and numir\n",
    "\n",
    "Before introducing grain, we will first implement the network using mir. Mir provides an n-dimensional slice value, and many functions for manipulating these slices. Mir is a generic framework for scientific computing; it does not know anything about computation graphs, or deep learning, or gradients. However we can easily use mir to fit a two-layer network to random data by manually implementing the forward and backward passes through the network using mir operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO hand-written 2 layer NN implementation like example/char_rnn_hand.d\n",
    "import mir.ndslice;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grain.autograd.Variable\n",
    "\n",
    "In the above examples, we had to manually implement both the forward and backward passes of our neural network. Manually implementing the backward pass is not a big deal for a small two-layer network, but can quickly get very hairy for large complex networks.\n",
    "\n",
    "Thankfully, we can use automatic differentiation to automate the computation of backward passes in neural networks. The autograd package in grain provides exactly this functionality. When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be `Variable`, and edges will be functions that produce output Variable from input Variable. In addition to Slice feature, Variable contains autograd history (simply previously forwarded `Function` object that also implements its backward function) as `grain.autograd.Backprop` type. Backpropagating through this graph then allows you to easily compute gradients. Moreover, unlike mir.ndslice only supports CPU memory slices, grain supports both CPU and CUDA memory slices.\n",
    "\n",
    "This sounds complicated, itâ€™s pretty simple to use in practice. Each Slice represents a node in a computational graph. If x is a Slice that has x.requiresGrad=True then x.grad is another memory holding the gradient of x with respect to some scalar value (and x.gradSlice hold a slice of the gradient).\n",
    "\n",
    "Here we use `grain.autograd.Variable` and autograd to implement our two-layer network; now we no longer need to manually implement the backward pass through the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable!(float, dim=0, HostStorage(T))(data=[106.517], shape=[], strides=[])\n",
       "[[0, 3.05342, -7.21415, -1.20809, 0.619979, -6.90166, -2.58402, -8.15106, -1.45499, -5.18359], [0, -6.59489, 15.5814, 3.64662, -1.70193, 15.0758, 7.4032, 18.2983, 3.03704, 13.2295], [0, -4.40347, 10.4038, -6.26731, -1.40514, 10.8085, 20.4142, 22.1453, -3.83554, 27.5899], [0, -1.34407, 3.17556, -2.63964, 1.51712, 4.207, 11.7398, 7.33803, -2.31011, 16.3218], [0, -4.8304, 11.4125, -8.64689, -1.79212, 12.3359, 22.0187, 24.0389, -3.36994, 28.6646], [0, 2.71623, -6.41748, 4.56563, 1.35962, -6.43335, -16.9046, -13.621, 2.12364, -21.2614], [0, -2.49811, 5.90216, -4.02021, 1.35197, 5.41387, 21.9194, 17.6362, -8.0067, 32.4302], [0, -1.09686, 2.59148, -4.9225, -0.267527, 3.66343, 5.73918, 6.18388, -0.560154, 6.48508], [0, 3.17372, -7.49838, -4.13456, -1.09865, -7.16112, -7.39533, -9.02854, 0.351404, -13.6707], [0, -3.13249, 7.40096, -3.23553, -1.48884, 8.36951, 10.2488, 9.38674, 2.66765, 11.2667]]\n",
       "\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// TODO fix empty line outputs\n",
    "import std.stdio;\n",
    "import grain;\n",
    "import mir.random;\n",
    "import mir.ndslice;\n",
    "{\n",
    "    // enable autograd mode\n",
    "    backprop = true;\n",
    "    alias ftype = float;\n",
    "    alias device = HostStorage; // DeviceStorage\n",
    "\n",
    "    auto learningRate = 1e-6;\n",
    "    auto batchSize = 6;\n",
    "    auto inputDim = 10;\n",
    "    auto hiddenDim = 10;\n",
    "    auto outputDim = 5;\n",
    "    \n",
    "    auto genRand(bool requiresGrad = false, size_t dim)(size_t[dim] s...) {\n",
    "        return iota(s)\n",
    "            .map!(_ => rand!ftype)\n",
    "            .slice\n",
    "            .variable(requiresGrad)\n",
    "            .to!device;\n",
    "    }\n",
    "\n",
    "    // create random tensors to hold input and outputs\n",
    "    auto x = genRand(batchSize, inputDim);\n",
    "    auto y = genRand(batchSize, outputDim);\n",
    "\n",
    "    auto w1 = genRand!true(inputDim, hiddenDim);\n",
    "    auto w2 = genRand!true(hiddenDim, outputDim);\n",
    "\n",
    "    // compose NN sequence like torch with UFCS\n",
    "    auto y_pred = x\n",
    "        .matMul(w1)\n",
    "        .relu\n",
    "        .matMul(w2);\n",
    "\n",
    "    auto loss = (y - y_pred).pow(2).sum;\n",
    "    loss.backward();\n",
    "    // see backprop start to end\n",
    "    writeln(loss);\n",
    "    writeln(w1.gradSlice);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable!(float, dim=2, HostStorage(T))(data=[0.3, 0.4, 0.5, 0.6], shape=[2, 2], strides=[2, 1])\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// numpy-like broadcasting is available\n",
    "static assert(!__traits(compiles, [[0.1f], [0.3f]].variable + [0.2f].variable));\n",
    "// but you need to make them same dimensions\n",
    "[[0.1f, 0.2f], [0.3f, 0.4f]].variable + [[0.2f]].variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining new autograd functions\n",
    "\n",
    "Under the hood, each primitive autograd operator is really two functions that operate on Variables. The forward function computes output Variables from input Variable. The backward function receives the gradient of the output Variables with respect to some scalar value, and computes the gradient of the input Variables with respect to that same scalar value.\n",
    "\n",
    "In grain, we can easily define our own autograd operator by defining a `Function` with `grain.functions.common.FunctionCommon` mixin and implementing the forward and backward functions. We can then use our new autograd operator by constructing an instance and calling it like a function, passing Variables containing input data.\n",
    "\n",
    "- you can find many examples in `grain.functions` module\n",
    "\n",
    "In this example we define our own custom autograd function for performing the ReLU nonlinearity, and use it to implement our two-layer network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/// rectified linear unit nonlinearity\n",
    "struct MyReLU(T, size_t dim) {\n",
    "\n",
    "    mixin FunctionCommon;\n",
    "    Variable!(T, dim, HostStorage) hx;\n",
    "\n",
    "    auto forward(Variable!(T, dim, HostStorage) x) {\n",
    "        import mir.ndslice : each;\n",
    "        this.hx = x.dup;\n",
    "        auto y = x.dup;\n",
    "        y.sliced.each!((ref a) {\n",
    "            if (a < 0)\n",
    "                a = 0;\n",
    "        });\n",
    "        return y;\n",
    "    }\n",
    "\n",
    "    auto backward(Variable!(T, dim, HostStorage) gy) {\n",
    "        auto gx = gy.dup;\n",
    "        foreach (i; 0 .. gx.data.length) {\n",
    "            if (this.hx.data[i] < 0.0)\n",
    "                gx.data[i] = 0.0;\n",
    "        }\n",
    "        return gx;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d",
   "language": "d",
   "name": "d"
  },
  "language_info": {
   "file_extension": ".d",
   "mimetype": "text/plain",
   "name": "D",
   "version": "2.081.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
